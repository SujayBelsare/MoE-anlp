# Configuration file for ANLP Assignment 3 - MoE Transformer

# Data Configuration
data:
  dataset_name: "EdinburghNLP/xsum"
  max_input_length: 512
  max_target_length: 64
  train_samples: null  # null for full dataset, or specify number
  val_samples: null
  test_samples: null

# Model Configuration
model:
  # General
  vocab_size: 50265  # BART tokenizer vocab size
  d_model: 512
  nhead: 8
  num_encoder_layers: 4
  num_decoder_layers: 4
  dim_feedforward: 2048
  dropout: 0.1
  max_seq_length: 512
  
  # MoE Specific
  num_experts: 8
  expert_capacity: null  # null for dynamic, or specify
  top_k: 2  # number of experts per token
  expert_hidden_dim: 2048
  
  # Load Balancer
  load_balance_loss_coef: 0.01
  use_load_balancer: true

# Training Configuration
training:
  batch_size: 4
  gradient_accumulation_steps: 4
  num_epochs: 10
  learning_rate: 0.00005
  weight_decay: 0.01
  warmup_steps: 1000
  seed: 42
  fp16: true
  save_steps: 1000
  eval_steps: 500
  logging_steps: 100
  save_total_limit: 3
  max_grad_norm: 0.5

# Baseline Models Configuration
baselines:
  bart:
    model_name: "facebook/bart-large-xsum"
    batch_size: 32  # Reduced from 256 to avoid OOM with multi-GPU
  
  encoder_decoder:
    model_name: "google-t5/t5-base"  # or t5-large
    batch_size: 32
    num_epochs: 3
    learning_rate: 0.000005
    use_peft: true
    lora_r: 8
    lora_alpha: 32
    lora_dropout: 0.1
    max_grad_norm: 0.5
    fp16: false
    bf16: true
  
  instruct:
    model_name: "meta-llama/Llama-3.2-1B-Instruct"
    batch_size: 16
    num_epochs: 3
    learning_rate: 0.000005
    use_peft: true
    lora_r: 8
    lora_alpha: 32
    lora_dropout: 0.1
    load_in_8bit: true  # Changed to true for 2x faster inference with minimal quality loss
    max_grad_norm: 0.5
    fp16: false
    bf16: true
    inference_batch_size: 8  # Reduced from 32 to 8 for faster per-batch generation


# Evaluation Configuration
evaluation:
  batch_size: 8  # Reduced from 64 to 8 for MoE models (use same as training for consistency)
  num_beams: 2
  length_penalty: 0.6
  max_length: 128  # Changed from 512 to 128 - summaries are typically much shorter
  min_length: 10
  no_repeat_ngram_size: 3

# Bonus Configuration
bonus:
  use_gqa: false  # Bonus 2
  num_kv_heads: 4  # for GQA
  use_lora_experts: false  # Bonus 3
  lora_rank: 8
  lora_alpha: 16

# Output Configuration
output:
  model_dir: "./models"
  results_dir: "./results"
  logs_dir: "./logs"
  visualizations_dir: "./visualizations"
  hf_hub_repo: null  # Set your HF repo name, e.g., "username/anlp-moe-xsum"

# Hardware Configuration
hardware:
  device: "cuda"  # cuda or cpu
  use_multi_gpu: true  # Use DataParallel for multi-GPU
  num_workers: 4
  pin_memory: true