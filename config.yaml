# Mode configuration
mode: train  # One of: baseline, train, evaluate

# Dataset configuration
dataset:
  name: EdinburghNLP/xsum
  max_source_length: 512
  max_target_length: 64

# Tokenizer configuration
tokenizer: facebook/bart-base

# Model configuration
model:
  d_model: 512
  n_heads: 8
  n_layers: 6
  d_ff: 2048
  dropout_rate: 0.1

# MoE configuration
moe:
  num_experts: 8
  top_k: 2
  load_balancer_weight: 0.01

# Training configuration
training:
  batch_size: 16
  learning_rate: 5e-5
  num_epochs: 10
  warmup_steps: 1000
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  train_samples: null  # Limit training samples (for debugging)
  val_samples: null    # Limit validation samples (for debugging)

# Debug configuration
debug:
  enabled: false
  train_samples: 100
  val_samples: 50
  num_epochs: 2

# Data loading
data:
  num_workers: 4

# Device configuration
device: cuda  # Will be automatically set based on availability

# Paths
paths:
  checkpoint_dir: checkpoints
  output_dir: outputs
  log_dir: logs

# HuggingFace Hub
huggingface:
  username: your-username  # Update with your HF username
  push_to_hub: false      # Whether to push models to HuggingFace Hub

# Weights & Biases configuration
wandb:
  enabled: true
  project: anlp-moe-xsum

# Checkpointing
checkpointing:
  save_every: 1        # Save checkpoint every N epochs
  save_best: true      # Save best model based on validation loss

# Random seed for reproducibility
seed: 42

# Baseline models
baseline_models:
  bart: facebook/bart-large-xsum
  t5:
    - google-t5/t5-base
    - google-t5/t5-large
    - google/pegasus-large
  instruction:
    - meta-llama/Llama-3.2-1B-Instruct
    - meta-llama/Llama-3.2-3B-Instruct
    - Qwen/Qwen2.5-3B-Instruct

# Evaluation
evaluation:
  num_beams: 4
  length_penalty: 2.0
  early_stopping: true